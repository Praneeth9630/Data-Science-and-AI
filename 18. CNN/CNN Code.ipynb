{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2366a5c3-90da-4e33-a8c2-68c2ac7e9e30",
   "metadata": {},
   "source": [
    "# Business Problem\n",
    "- Classify the images & predict on future on data (whether the image belongs to which class) with maximum accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8c9865-bef6-471e-9c29-91aade173907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d130b70-0577-41e3-8d03-d08fa518a63c",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "- they have given multiple images of cats & dogs\n",
    "- cat images are in 1 folder & dog images are in 1 folder\n",
    "- Since, we have 2 classes, its a Binary Image Classification Project\n",
    "- the given images are of diffrent sizes(different shapes)\n",
    "\n",
    "# Data (Image) preprocessing\n",
    "\n",
    "**Load the image data & Image Data preprocessing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43dcfcfe-e65e-44b8-923a-be3380267f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68fe5459-1cee-45f9-9429-07b5b2bd8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1/255) # pixel intensity bought between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd53491-bc7b-4717-a061-db18857af978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8048 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen. flow_from_directory('dataset/training_set',       # flow_from_directory means from the folder take one by one.\n",
    "                                                  target_size = (64, 64),\n",
    "                                                  class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d7bd47-f54c-46ee-894b-954ba4e23f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1555247-cf8b-4ada-a4fe-d3cbaf4b9e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "\n",
    "test_set = test_datagen. flow_from_directory('dataset/test_set',\n",
    "                                             target_size = (64, 64),\n",
    "                                             class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c6a6d-48ed-4a59-9f72-bbda44a24a7e",
   "metadata": {},
   "source": [
    "# Modelling - Convolution Neural Network\n",
    "- **Initialising the CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f099c2a0-22da-4078-9a9f-27ad71387aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774614d-2ef6-4055-a7b7-b0afd22702f8",
   "metadata": {},
   "source": [
    "**Step 1 - Convolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec2eef04-1005-41e4-beaf-aef358bc1195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D\n",
    "\n",
    "classifier.add(Conv2D(input_shape=[64, 64, 3],                     # 64,64,3 is every image size. where 64,64 is pixels, 3 means coloured image, it will be 2 if black and white image\n",
    "                      filters=32,                                  #max no.of filters\n",
    "                      kernel_size=3,                               #fixed --> It is filter size.\n",
    "                      activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5f57a-d0ef-43b3-86c8-e912bcb3f4af",
   "metadata": {},
   "source": [
    "**Step 2 - Max Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a00e93d-e4a9-4a9b-a466-a21ccebd3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=2,strides=2))   # Stride of two means there is no overlapping with before pool as its size is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5482e-f8eb-4aec-b757-05640efcd82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(input_shape=[64, 64, 3],                  #\n",
    "                      filters=32,                               #max no.of filters\n",
    "                      kernel_size=3,                            #fixed\n",
    "                      activation='relu'))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad587d-6ff9-4817-8f0a-a4ffeb2c3665",
   "metadata": {},
   "source": [
    "**Step 3 - Flattening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fff091f3-06fb-4948-8b41-b310478cce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23d75a-8108-4552-8c8b-29e3c1f21128",
   "metadata": {},
   "source": [
    "**Step 4 - Full Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cddfdf2-9238-4222-927d-e450019a1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras. layers import Dense\n",
    "\n",
    "# hidden Layer with 128 neurons\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "# Output Layer with 1 neuron\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0a708-ad09-4567-a527-20f7ec264ebe",
   "metadata": {},
   "source": [
    "**Training the CNN Model with train data & Testing the model with test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d85aa2fd-c674-486e-952c-e35490e9368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam',\n",
    "                   loss = 'binary_crossentropy',\n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "857d7539-b692-4401-9451-0cd4efa38336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 798ms/step - accuracy: 0.5947 - loss: 0.7024 - val_accuracy: 0.6725 - val_loss: 0.6093\n",
      "Epoch 2/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 111ms/step - accuracy: 0.7502 - loss: 0.5199 - val_accuracy: 0.7120 - val_loss: 0.5760\n",
      "Epoch 3/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 118ms/step - accuracy: 0.8143 - loss: 0.4141 - val_accuracy: 0.7305 - val_loss: 0.5835\n",
      "Epoch 4/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 109ms/step - accuracy: 0.8551 - loss: 0.3352 - val_accuracy: 0.7205 - val_loss: 0.6267\n",
      "Epoch 5/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 137ms/step - accuracy: 0.8881 - loss: 0.2778 - val_accuracy: 0.7255 - val_loss: 0.6907\n",
      "Epoch 6/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - accuracy: 0.9358 - loss: 0.1772 - val_accuracy: 0.7185 - val_loss: 0.7336\n",
      "Epoch 7/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 114ms/step - accuracy: 0.9597 - loss: 0.1249 - val_accuracy: 0.7290 - val_loss: 0.8843\n",
      "Epoch 8/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 119ms/step - accuracy: 0.9785 - loss: 0.0750 - val_accuracy: 0.7250 - val_loss: 0.9163\n",
      "Epoch 9/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 112ms/step - accuracy: 0.9925 - loss: 0.0429 - val_accuracy: 0.7035 - val_loss: 1.1811\n",
      "Epoch 10/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 118ms/step - accuracy: 0.9960 - loss: 0.0276 - val_accuracy: 0.7185 - val_loss: 1.2134\n",
      "Epoch 11/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 118ms/step - accuracy: 0.9963 - loss: 0.0192 - val_accuracy: 0.7155 - val_loss: 1.3001\n",
      "Epoch 12/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 122ms/step - accuracy: 0.9884 - loss: 0.0419 - val_accuracy: 0.7110 - val_loss: 1.4520\n",
      "Epoch 13/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 120ms/step - accuracy: 0.9950 - loss: 0.0189 - val_accuracy: 0.7000 - val_loss: 1.4937\n",
      "Epoch 14/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - accuracy: 0.9958 - loss: 0.0218 - val_accuracy: 0.7205 - val_loss: 1.6078\n",
      "Epoch 15/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 104ms/step - accuracy: 0.9999 - loss: 0.0039 - val_accuracy: 0.7255 - val_loss: 1.6966\n",
      "Epoch 16/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7230 - val_loss: 1.7765\n",
      "Epoch 17/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 7.7408e-04 - val_accuracy: 0.7260 - val_loss: 1.8204\n",
      "Epoch 18/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 6.2196e-04 - val_accuracy: 0.7230 - val_loss: 1.8681\n",
      "Epoch 19/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 106ms/step - accuracy: 1.0000 - loss: 5.0368e-04 - val_accuracy: 0.7275 - val_loss: 1.8983\n",
      "Epoch 20/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 4.1482e-04 - val_accuracy: 0.7260 - val_loss: 1.9303\n",
      "Epoch 21/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 3.4470e-04 - val_accuracy: 0.7285 - val_loss: 1.9514\n",
      "Epoch 22/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 3.0991e-04 - val_accuracy: 0.7260 - val_loss: 1.9840\n",
      "Epoch 23/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 2.4245e-04 - val_accuracy: 0.7285 - val_loss: 2.0089\n",
      "Epoch 24/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 2.1169e-04 - val_accuracy: 0.7280 - val_loss: 2.0306\n",
      "Epoch 25/25\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 1.7644e-04 - val_accuracy: 0.7270 - val_loss: 2.0621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x227e31a1910>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4a8c8-c980-4818-851d-18012ee6d583",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "- **Making a single prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bb064dd-f870-47e8-9280-3758817fdbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "285814d0-12af-41aa-bca7-ace3e6d1c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Dog\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "test_image = Image.open(\"dataset/single_prediction/cat_or_dog_1.jpg\")\n",
    "\n",
    "# Data Preprocessing\n",
    "test_image = test_image.resize((64,64))\n",
    "test_image = np.array(test_image)\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "\n",
    "# Prediction\n",
    "result= classifier.predict(test_image)\n",
    "\n",
    "# Evaluation\n",
    "if result[0][0] == 1:\n",
    "    print(\"Dog\")\n",
    "else:\n",
    "    print(\"Cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57faff02-1376-42ee-8561-3c71d8b6c4e7",
   "metadata": {},
   "source": [
    "# There are four things not fixed in CNN\n",
    "1) Pixel Size\n",
    "2) No of Hidden layers\n",
    "3) No of Hidden layer neurons\n",
    "4) Epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa152e93-0a9d-4979-8a64-59873c0a08b2",
   "metadata": {},
   "source": [
    "- **If the model is overfitted we should**\n",
    " \n",
    "        - option-1:- reduce the number of filters.\n",
    "  \n",
    "        - option-2:- Reduce the no of hidden layers and hidden layer neurons.\n",
    "  \n",
    "        - option-3:- Reduce the no of epochs.\n",
    "  \n",
    "- **If the model is underfitted it means dataset is insufficient.**\n",
    "- **If accuracy is low then**\n",
    "  \n",
    "        - option-1:- Provide high quality images & increase pixel size.\n",
    "  \n",
    "        - option-2:- Remove the low quality images.\n",
    "  \n",
    "        - option-3:- Add more Hidden Layers\n",
    "  \n",
    "        - option-4:- Add more epochs\n",
    "  \n",
    "        - option-5:- Add one more convolution layer(That is adding convolution layer after max pooling with again maxpooling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc347d-4b25-46bb-9281-1010d8f9d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we used option 5 above this is adding convolution layer after max pooling with again maxpooling.\n",
    "\n",
    "classifier.add(Conv2D(input_shape=[64, 64, 3],                  #\n",
    "                      filters=32,                               #max no.of filters\n",
    "                      kernel_size=3,                            #fixed\n",
    "                      activation='relu'))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=2,strides=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
